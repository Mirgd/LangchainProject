{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b836e543",
   "metadata": {},
   "source": [
    "##Hide warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e0f0ed0-8772-47a2-bdb3-f13c0861442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38a391e",
   "metadata": {},
   "source": [
    "##Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe0aa02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d4be625-ef8b-429f-921c-7bf85df83edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the superfolder path to sys.path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from keys import LANGCHAIN_API_KEY , HUGGINGFACEHUB_API_TOKEN\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e7178a",
   "metadata": {},
   "source": [
    "#Simple Quiry-Answer Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a5d2dff-cdac-417e-9cec-320272adfdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "007c3db2-d58e-41e4-989d-030a3c780e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_2952\\3173133658.py:2: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  llm=HuggingFaceHub(repo_id=\"meta-llama/Llama-3.2-1B\",\n",
      "C:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_2952\\3173133658.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(prompt=prompt,\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, \n",
    "                     llm=HuggingFaceHub(repo_id=\"meta-llama/Llama-3.2-1B\", \n",
    "                                        model_kwargs={\"Temperature\":0, \n",
    "                                                      \n",
    "                                                      \"max_length\":64}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f7f1c1",
   "metadata": {},
   "source": [
    "##The result of using a text generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "270feba9-c2c2-4b9e-a24a-dea4457601bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_2952\\579392319.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(llm_chain.run(question))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of Egypt?\n",
      "\n",
      "Answer:  Cairo\n"
     ]
    }
   ],
   "source": [
    "question=\"What is the capital of Egypt?\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7053f4a4-b585-45d9-b520-cd53e113ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain2 = LLMChain(prompt=prompt, \n",
    "                     llm=HuggingFaceHub(repo_id=\"google/flan-t5-large\", \n",
    "                                        model_kwargs={\"max_length\":64}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b92b34",
   "metadata": {},
   "source": [
    "##The result of usinf text2text generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bec435e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ajo@aol.com\n"
     ]
    }
   ],
   "source": [
    "question=\"what is the email of the employee Alice Johnson\"\n",
    "print(llm_chain2.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0f2ca62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scott mccullough\n"
     ]
    }
   ],
   "source": [
    "question=\"Who are the leadership of the marketing team?\"\n",
    "print(llm_chain2.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f419831",
   "metadata": {},
   "source": [
    "#Retrival Augmented Generation (RAG) Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb18fa13",
   "metadata": {},
   "source": [
    "##step 1: Load local documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49394121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:50<00:00, 50.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader(\"../data\",glob=\"*.pdf\",show_progress=True)\n",
    "doc = loader.load()\n",
    "print(len(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34564ad7",
   "metadata": {},
   "source": [
    "##step 2: Split large documents into smaller chunks for embedding and retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0622abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 document(s) into 35 chunks.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "chunks = text_splitter.split_documents(doc)\n",
    "print(f\"Split {len(doc)} document(s) into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 3: Convert document chunks into vector embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72fa667",
   "metadata": {},
   "source": [
    "##Step 4: Vector Stores: Store the embeddings for efficient retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93bab3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 35 chunks to chroma.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_2952\\3231597218.py:15: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import shutil\n",
    "\n",
    "CHROMA_PATH =\"chroma\"\n",
    "\n",
    "# Clear out the database first if exist.\n",
    "if os.path.exists(CHROMA_PATH):\n",
    "    shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "# Create a new DB from the documents.\n",
    "db = Chroma.from_documents(\n",
    "    chunks, HuggingFaceEmbeddings(), persist_directory=CHROMA_PATH\n",
    ")\n",
    "db.persist()\n",
    "print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeaefbe",
   "metadata": {},
   "source": [
    "##step 5: Retrievers: Configure a retriever to fetch relevant documents from the vector store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2043cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_text = \"what is the email of the employee Alice Johnson\"\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "retrieved_docs = retriever.invoke(query_text)\n",
    "len(retrieved_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad57eba9",
   "metadata": {},
   "source": [
    "##format the retrived chuncks of a document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b817007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Use the following retrieved context to answer the query:\n",
      "\n",
      "Employee Details Alice Johnson Software Engineer 03-15\n",
      "\n",
      "Development alice.johnson@infratech.com 2022-\n",
      "\n",
      "---\n",
      "\n",
      "Diana Williams\n",
      "\n",
      "HR Specialist Human Resources\n",
      "\n",
      "diana.williams@infratech.com\n",
      "\n",
      "2019-09-01\n",
      "\n",
      "---\n",
      "\n",
      "Employee Information Report This document contains detailed information about the employees of InfraTech. It includes their names, roles, departments, contact information, and hire dates. In this\n",
      "\n",
      "Query: what is the email of the employee Alice Johnson\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Use the following retrieved context to answer the query:\n",
    "\n",
    "{context}\n",
    "\n",
    "Query: {query}\n",
    "{answer}\n",
    "\"\"\"\n",
    "\n",
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "prompt_format = prompt_template.format(context=context_text, query=query_text, answer=\"\")\n",
    "print(prompt_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5309bc3",
   "metadata": {},
   "source": [
    "##step 6: LLM Integration: integerate the retrived chuncks in the LLM query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c714aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Use the following retrieved context to answer the query:\n",
      "\n",
      "Employee Details Alice Johnson Software Engineer 03-15\n",
      "\n",
      "Development alice.johnson@infratech.com 2022-\n",
      "\n",
      "---\n",
      "\n",
      "Diana Williams\n",
      "\n",
      "HR Specialist Human Resources\n",
      "\n",
      "diana.williams@infratech.com\n",
      "\n",
      "2019-09-01\n",
      "\n",
      "---\n",
      "\n",
      "Employee Information Report This document contains detailed information about the employees of InfraTech. It includes their names, roles, departments, contact information, and hire dates. In this\n",
      "\n",
      "Query: what is the email of the employee Alice Johnson\n",
      "Response: alice.johnson@infratech.com\n",
      "Sources: ['..\\\\data\\\\InfraTech_Employee_Information_Report.pdf', '..\\\\data\\\\InfraTech_Employee_Information_Report.pdf', '..\\\\data\\\\InfraTech_Employee_Information_Report.pdf']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the LLM chain\n",
    "# Initialize the LLM from Hugging Face Hub\n",
    "llm_chain3 = LLMChain(prompt=prompt_template, \n",
    "                      llm=HuggingFaceHub(\n",
    "                        repo_id=\"google/flan-t5-large\",\n",
    "                        model_kwargs={\"max_length\": 64, \"temperature\": 0.5})  \n",
    "                      )\n",
    "\n",
    "# Run the chain with context and query\n",
    "response_text = llm_chain3.run({\"context\": context_text, \"query\": query_text, \"answer\":\"\"})\n",
    "\n",
    "# Extract sources from metadata\n",
    "sources = [doc.metadata.get(\"source\", None) for doc in retrieved_docs]\n",
    "\n",
    "# Format the response\n",
    "formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "prompt_format = prompt_template.format(context=context_text, query=query_text, answer=formatted_response)\n",
    "print(prompt_format)\n",
    "#print(formatted_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a410f7d0",
   "metadata": {},
   "source": [
    "#Combine the RAG components in a function for easy testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fc4b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RAG_query(query_text):\n",
    "\n",
    "    #Retrieve documents\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "    retrieved_docs = retriever.get_relevant_documents(query_text)\n",
    "\n",
    "    #Prepare context\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    #Define prompt template\n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "    Use the following retrieved context to answer the query:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Query: {query}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    #Create the prompt template\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "\n",
    "    #Initialize the LLM\n",
    "    llm_chain = LLMChain(\n",
    "        prompt=prompt_template,\n",
    "        llm=HuggingFaceHub(\n",
    "            repo_id=\"google/flan-t5-large\",\n",
    "            model_kwargs={\"max_length\": 64, \"temperature\": 0}\n",
    "        )\n",
    "    )\n",
    "\n",
    "    #Run the LLM chain with context and query\n",
    "    response_text = llm_chain.run({\"context\": context_text, \"query\": query_text})\n",
    "\n",
    "    #Extract the most relvent source\n",
    "    sources = retrieved_docs[0].metadata.get(\"source\", \"Unknown\") if retrieved_docs else \"No sources available\"\n",
    "\n",
    "    #Format the final response\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "\n",
    "    return formatted_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a49102",
   "metadata": {},
   "source": [
    "#Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fce2882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Bob Smith and Peter Davis\n",
      "Sources: ..\\data\\InfraTech_Employee_Information_Report.pdf\n"
     ]
    }
   ],
   "source": [
    "query_text = \"Who are the leadership of the marketing team?\"\n",
    "response = RAG_query(query_text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfedf5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
