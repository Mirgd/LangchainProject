{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b836e543",
   "metadata": {},
   "source": [
    "##Hide warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e0f0ed0-8772-47a2-bdb3-f13c0861442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38a391e",
   "metadata": {},
   "source": [
    "##Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe0aa02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras==2.11.0 (from -r requirements.txt (line 1))\n",
      "  Using cached keras-2.11.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting tensorflow<3.0.0,>=2.10.0 (from -r requirements.txt (line 2))\n",
      "  Using cached tensorflow-2.18.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: libmagic in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (1.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (0.3.11)\n",
      "Requirement already satisfied: langchain_community in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from -r requirements.txt (line 5)) (0.3.10)\n",
      "Requirement already satisfied: langchain_huggingface in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from -r requirements.txt (line 6)) (0.1.2)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (1.0.1)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from -r requirements.txt (line 8)) (0.26.5)\n",
      "Requirement already satisfied: onnxruntime==1.17.1 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from -r requirements.txt (line 9)) (1.17.1)\n",
      "Requirement already satisfied: chromadb==0.5.0 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from -r requirements.txt (line 11)) (0.5.0)\n",
      "Requirement already satisfied: tiktoken==0.7.0 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from -r requirements.txt (line 12)) (0.7.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from -r requirements.txt (line 13)) (4.67.1)\n",
      "Requirement already satisfied: python-magic-bin==0.4.14 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from -r requirements.txt (line 14)) (0.4.14)\n",
      "Requirement already satisfied: torch in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from -r requirements.txt (line 15)) (2.5.1)\n",
      "Requirement already satisfied: transformers>=4.31.0 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from -r requirements.txt (line 16)) (4.47.0)\n",
      "Requirement already satisfied: unstructured[pdf] in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from -r requirements.txt (line 10)) (0.16.11)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from onnxruntime==1.17.1->-r requirements.txt (line 9)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from onnxruntime==1.17.1->-r requirements.txt (line 9)) (24.3.25)\n",
      "Requirement already satisfied: numpy>=1.26.4 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from onnxruntime==1.17.1->-r requirements.txt (line 9)) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from onnxruntime==1.17.1->-r requirements.txt (line 9)) (24.2)\n",
      "Requirement already satisfied: protobuf in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from onnxruntime==1.17.1->-r requirements.txt (line 9)) (5.29.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from onnxruntime==1.17.1->-r requirements.txt (line 9)) (1.13.1)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (1.2.2.post1)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (2.32.3)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (2.9.2)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (0.115.6)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.5.0->-r requirements.txt (line 11)) (0.32.1)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (3.7.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (4.12.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (0.49b2)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (1.28.2)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (0.21.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (1.68.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (4.2.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (0.15.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (31.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (5.0.1)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (3.10.12)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from tiktoken==0.7.0->-r requirements.txt (line 12)) (2024.11.6)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow<3.0.0,>=2.10.0->-r requirements.txt (line 2))\n",
      "  Using cached tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.18.0->tensorflow<3.0.0,>=2.10.0->-r requirements.txt (line 2))\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.18.0->tensorflow<3.0.0,>=2.10.0->-r requirements.txt (line 2))\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.18.0->tensorflow<3.0.0,>=2.10.0->-r requirements.txt (line 2))\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.18.0->tensorflow<3.0.0,>=2.10.0->-r requirements.txt (line 2))\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.18.0->tensorflow<3.0.0,>=2.10.0->-r requirements.txt (line 2))\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.18.0->tensorflow<3.0.0,>=2.10.0->-r requirements.txt (line 2))\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3.0.0,>=2.10.0->-r requirements.txt (line 2)) (75.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3.0.0,>=2.10.0->-r requirements.txt (line 2)) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.18.0->tensorflow<3.0.0,>=2.10.0->-r requirements.txt (line 2))\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\win10\\desktop\\langchianproject\\langchainproject\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3.0.0,>=2.10.0->-r requirements.txt (line 2)) (1.17.0)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow<3.0.0,>=2.10.0->-r requirements.txt (line 2))\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "INFO: pip is looking at multiple versions of tensorflow-intel to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow<3.0.0,>=2.10.0 (from -r requirements.txt (line 2))\n",
      "  Using cached tensorflow-2.17.1-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.17.1 (from tensorflow<3.0.0,>=2.10.0->-r requirements.txt (line 2))\n",
      "  Using cached tensorflow_intel-2.17.1-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting h5py>=3.10.0 (from tensorflow-intel==2.17.1->tensorflow<3.0.0,>=2.10.0->-r requirements.txt (line 2))\n",
      "  Using cached h5py-3.12.1-cp312-cp312-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.3.1 (from tensorflow-intel==2.17.1->tensorflow<3.0.0,>=2.10.0->-r requirements.txt (line 2))\n",
      "  Using cached ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting protobuf (from onnxruntime==1.17.1->-r requirements.txt (line 9))\n",
      "  Using cached protobuf-4.25.5-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting tensorboard<2.18,>=2.17 (from tensorflow-intel==2.17.1->tensorflow<3.0.0,>=2.10.0->-r requirements.txt (line 2))\n",
      "  Using cached tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow<3.0.0,>=2.10.0 (from -r requirements.txt (line 2))\n",
      "  Using cached tensorflow-2.17.0-cp312-cp312-win_amd64.whl.metadata (3.2 kB)\n",
      "Collecting tensorflow-intel==2.17.0 (from tensorflow<3.0.0,>=2.10.0->-r requirements.txt (line 2))\n",
      "  Using cached tensorflow_intel-2.17.0-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting tensorflow<3.0.0,>=2.10.0 (from -r requirements.txt (line 2))\n",
      "  Using cached tensorflow-2.16.2-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.16.2 (from tensorflow<3.0.0,>=2.10.0->-r requirements.txt (line 2))\n",
      "  Using cached tensorflow_intel-2.16.2-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow-intel==2.16.2->tensorflow<3.0.0,>=2.10.0->-r requirements.txt (line 2))\n",
      "  Using cached ml_dtypes-0.3.2-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow-intel==2.16.2->tensorflow<3.0.0,>=2.10.0->-r requirements.txt (line 2))\n",
      "  Using cached tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow<3.0.0,>=2.10.0 (from -r requirements.txt (line 2))\n",
      "  Using cached tensorflow-2.16.1-cp312-cp312-win_amd64.whl.metadata (3.5 kB)\n",
      "Collecting tensorflow-intel==2.16.1 (from tensorflow<3.0.0,>=2.10.0->-r requirements.txt (line 2))\n",
      "  Using cached tensorflow_intel-2.16.1-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "\n",
      "The conflict is caused by:\n",
      "    The user requested keras==2.11.0\n",
      "    tensorflow-intel 2.16.1 depends on keras>=3.0.0\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot install keras==2.11.0 and tensorflow because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4be625-ef8b-429f-921c-7bf85df83edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keys import LANGCHAIN_API_KEY , HUGGINGFACEHUB_API_TOKEN\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e7178a",
   "metadata": {},
   "source": [
    "#Simple Quiry-Answer Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a5d2dff-cdac-417e-9cec-320272adfdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "007c3db2-d58e-41e4-989d-030a3c780e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_16716\\3173133658.py:2: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  llm=HuggingFaceHub(repo_id=\"meta-llama/Llama-3.2-1B\",\n",
      "C:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_16716\\3173133658.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(prompt=prompt,\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, \n",
    "                     llm=HuggingFaceHub(repo_id=\"meta-llama/Llama-3.2-1B\", \n",
    "                                        model_kwargs={\"Temperature\":0, \n",
    "                                                      \n",
    "                                                      \"max_length\":64}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f7f1c1",
   "metadata": {},
   "source": [
    "##The result of using a text generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "270feba9-c2c2-4b9e-a24a-dea4457601bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_16716\\579392319.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(llm_chain.run(question))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of Egypt?\n",
      "\n",
      "Answer:  Cairo\n"
     ]
    }
   ],
   "source": [
    "question=\"What is the capital of Egypt?\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7053f4a4-b585-45d9-b520-cd53e113ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain2 = LLMChain(prompt=prompt, \n",
    "                     llm=HuggingFaceHub(repo_id=\"google/flan-t5-large\", \n",
    "                                        model_kwargs={\"max_length\":64}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b92b34",
   "metadata": {},
   "source": [
    "##The result of usinf text2text generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bec435e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ajo@aol.com\n"
     ]
    }
   ],
   "source": [
    "question=\"what is the email of the employee Alice Johnson\"\n",
    "print(llm_chain2.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11514cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "January 1, 2017\n"
     ]
    }
   ],
   "source": [
    "question=\"what is Liam White hiring date?\"\n",
    "print(llm_chain2.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f419831",
   "metadata": {},
   "source": [
    "#Retrival Augmented Generation (RAG) Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb18fa13",
   "metadata": {},
   "source": [
    "##step 1: Load local documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49394121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:20<00:00, 20.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader(\"data\",glob=\"*.pdf\",show_progress=True)\n",
    "doc = loader.load()\n",
    "print(len(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34564ad7",
   "metadata": {},
   "source": [
    "##step 2: Split large documents into smaller chunks for embedding and retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0622abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 document(s) into 20 chunks.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "chunks = text_splitter.split_documents(doc)\n",
    "print(f\"Split {len(doc)} document(s) into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 3: Convert document chunks into vector embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72fa667",
   "metadata": {},
   "source": [
    "##Step 4: Vector Stores: Store the embeddings for efficient retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93bab3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 20 chunks to chroma.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_16716\\3231597218.py:15: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import shutil\n",
    "\n",
    "CHROMA_PATH =\"chroma\"\n",
    "\n",
    "# Clear out the database first if exist.\n",
    "if os.path.exists(CHROMA_PATH):\n",
    "    shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "# Create a new DB from the documents.\n",
    "db = Chroma.from_documents(\n",
    "    chunks, HuggingFaceEmbeddings(), persist_directory=CHROMA_PATH\n",
    ")\n",
    "db.persist()\n",
    "print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeaefbe",
   "metadata": {},
   "source": [
    "##step 5: Retrievers: Configure a retriever to fetch relevant documents from the vector store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2043cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_text = \"what is the email of the employee Alice Johnson\"\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "retrieved_docs = retriever.invoke(query_text)\n",
    "len(retrieved_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad57eba9",
   "metadata": {},
   "source": [
    "##format the retrived chuncks of a document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b817007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Use the following retrieved context to answer the query:\n",
      "\n",
      "Employee Details Employ ee Name\n",
      "\n",
      "Role\n",
      "\n",
      "Departme nt\n",
      "\n",
      "Email Address\n",
      "\n",
      "Hire Date\n",
      "\n",
      "Alice Johnson\n",
      "\n",
      "Software Engineer\n",
      "\n",
      "Developm ent\n",
      "\n",
      "alice.johnson@infra tech.com\n",
      "\n",
      "2022-03-15\n",
      "\n",
      "Bob Smith\n",
      "\n",
      "Marketing Manager\n",
      "\n",
      "---\n",
      "\n",
      "Employee Information Report This document contains detailed information about the employees of InfraTech. It includes their names, roles, departments, contact information, and hire dates. In this\n",
      "\n",
      "---\n",
      "\n",
      "Marketing Manager\n",
      "\n",
      "Marketing bob.smith@infratec h.com\n",
      "\n",
      "2020-07-23\n",
      "\n",
      "Charlie Brown\n",
      "\n",
      "Project Manager\n",
      "\n",
      "Operations charlie.brown@infr\n",
      "\n",
      "atech.com\n",
      "\n",
      "2021-05-10\n",
      "\n",
      "Diana William s\n",
      "\n",
      "HR Specialist\n",
      "\n",
      "Query: what is the email of the employee Alice Johnson\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Use the following retrieved context to answer the query:\n",
    "\n",
    "{context}\n",
    "\n",
    "Query: {query}\n",
    "{answer}\n",
    "\"\"\"\n",
    "\n",
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "prompt_format = prompt_template.format(context=context_text, query=query_text, answer=\"\")\n",
    "print(prompt_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5309bc3",
   "metadata": {},
   "source": [
    "##step 6: LLM Integration: integerate the retrived chuncks in the LLM query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c714aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Use the following retrieved context to answer the query:\n",
      "\n",
      "Employee Details Employ ee Name\n",
      "\n",
      "Role\n",
      "\n",
      "Departme nt\n",
      "\n",
      "Email Address\n",
      "\n",
      "Hire Date\n",
      "\n",
      "Alice Johnson\n",
      "\n",
      "Software Engineer\n",
      "\n",
      "Developm ent\n",
      "\n",
      "alice.johnson@infra tech.com\n",
      "\n",
      "2022-03-15\n",
      "\n",
      "Bob Smith\n",
      "\n",
      "Marketing Manager\n",
      "\n",
      "---\n",
      "\n",
      "Employee Information Report This document contains detailed information about the employees of InfraTech. It includes their names, roles, departments, contact information, and hire dates. In this\n",
      "\n",
      "---\n",
      "\n",
      "Marketing Manager\n",
      "\n",
      "Marketing bob.smith@infratec h.com\n",
      "\n",
      "2020-07-23\n",
      "\n",
      "Charlie Brown\n",
      "\n",
      "Project Manager\n",
      "\n",
      "Operations charlie.brown@infr\n",
      "\n",
      "atech.com\n",
      "\n",
      "2021-05-10\n",
      "\n",
      "Diana William s\n",
      "\n",
      "HR Specialist\n",
      "\n",
      "Query: what is the email of the employee Alice Johnson\n",
      "Response: alice.johnson@infra tech.com\n",
      "Sources: ['data\\\\InfraTech_Employee_Information_Report.pdf', 'data\\\\InfraTech_Employee_Information_Report.pdf', 'data\\\\InfraTech_Employee_Information_Report.pdf']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the LLM chain\n",
    "# Initialize the LLM from Hugging Face Hub\n",
    "llm_chain3 = LLMChain(prompt=prompt_template, \n",
    "                      llm=HuggingFaceHub(\n",
    "                        repo_id=\"google/flan-t5-large\",\n",
    "                        model_kwargs={\"max_length\": 64, \"temperature\": 0.5})  \n",
    "                      )\n",
    "\n",
    "# Run the chain with context and query\n",
    "response_text = llm_chain3.run({\"context\": context_text, \"query\": query_text, \"answer\":\"\"})\n",
    "\n",
    "# Extract sources from metadata\n",
    "sources = [doc.metadata.get(\"source\", None) for doc in retrieved_docs]\n",
    "\n",
    "# Format the response\n",
    "formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "prompt_format = prompt_template.format(context=context_text, query=query_text, answer=formatted_response)\n",
    "print(prompt_format)\n",
    "#print(formatted_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a410f7d0",
   "metadata": {},
   "source": [
    "#Combine the RAG components in a function for easy testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0fc4b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RAG_query(query_text):\n",
    "\n",
    "    #Retrieve documents\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "    retrieved_docs = retriever.get_relevant_documents(query_text)\n",
    "\n",
    "    #Prepare context\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    #Define prompt template\n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "    Use the following retrieved context to answer the query:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Query: {query}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    #Create the prompt template\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "\n",
    "    #Initialize the LLM\n",
    "    llm_chain = LLMChain(\n",
    "        prompt=prompt_template,\n",
    "        llm=HuggingFaceHub(\n",
    "            repo_id=\"google/flan-t5-large\",\n",
    "            model_kwargs={\"max_length\": 64, \"temperature\": 0.5}\n",
    "        )\n",
    "    )\n",
    "\n",
    "    #Run the LLM chain with context and query\n",
    "    response_text = llm_chain.run({\"context\": context_text, \"query\": query_text})\n",
    "\n",
    "    #Extract the most relvent source\n",
    "    sources = retrieved_docs[0].metadata.get(\"source\", \"Unknown\") if retrieved_docs else \"No sources available\"\n",
    "\n",
    "    #Format the final response\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "\n",
    "    return formatted_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a49102",
   "metadata": {},
   "source": [
    "#Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7fce2882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Bob Smith and Peter Davis\n",
      "Sources: data\\InfraTech_Employee_Information_Report.pdf\n"
     ]
    }
   ],
   "source": [
    "query_text = \"Who are the marketing team?\"\n",
    "response = RAG_query(query_text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfedf5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
